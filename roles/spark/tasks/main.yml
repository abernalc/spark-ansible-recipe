---
- name: Update repositories cache
  yum:
    update_cache: yes

- name: Install vim java epel-release python-pip bzip2 git
  yum:
    name: 
      - vim
      - java
      - epel-release
      - python-pip
      - bzip2
      - git
    state: present

- name: Add spark user
  user:
    name: "{{ spark_user }}"
    create_home: true
    home: "{{ spark_user_home }}"

- name: Check {{ spark_user_home }} permissions
  file:
    path: "{{ spark_user_home }}"
    state: directory
    mode: 0766

- name: Check if the {{ spark_base_path }} exists
  file:
    path: "{{ spark_base_path }}"
    state: directory

- name: Download carde repository
  git:
    repo: "{{ carde_repository }}"
    dest: "{{ carde_repository_local }}"

- name: Curl the anaconda file
  get_url:
    url: "{{ anaconda_file_external_url }}"
    dest: "{{ anaconda_sh_file }}"
    owner: "{{ spark_user }}"
    mode: 0777

- name: Make sure all under the {{ spark_user_home }} is owned by {{ spark_user }}
  file:
    path: "{{ spark_user_home }}"
    state: directory
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"
    recurse: yes

- name: install pexpect
  pip:
    name: pexpect

- name: Install anaconda
  become: yes
  become_user: "{{ spark_user }}"
  command: /bin/bash -c "{{ anaconda_sh_file }} -b" 

- name: Untar the {{ spark_package }}
  unarchive:
    src: "{{ spark_package }}"
    dest: "{{ spark_base_path }}"
    remote_src: yes
    extra_opts: [--strip-components=1]
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"
  tags:
     - untar

- name: Setup spark environment variables
  blockinfile:
    path: "{{ bash_file }}"
    marker: "# {mark} SPARK environment variables"
    block: |
          SPARK_HOME={{ spark_base_path }}
          ANACONDA_HOME={{ anaconda_home }}
          export PATH=$SPARK_HOME/bin:$ANACONDA_HOME/bin:$PATH
        
    state: present

- name: create the {{ spark_env_file }} from the template
  copy: 
    src: "{{ spark_env_template }}"
    dest: "{{ spark_env_file }}"
    remote_src: yes
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"

- name: Setup {{ spark_local_ip  }} 
  blockinfile:
    path: "{{ spark_env_file }}"
    marker: "# {mark} hosts"
    block: |
          {{ spark_local_ip  }}={{ ansible_default_ipv4.address }}
    state: present

- name: Setup pyspark environment variables
  blockinfile:
    path: "{{ spark_env_file }}"
    marker: "# {mark} pyspark_python and driver"
    block: |
          {{ pyspark_python }}={{ pyspark_python_location }}
          {{ pyspark_driver_python }}={{ pyspark_driver_python_location }}
    state: present
  tags:
    - pyspark

- name: Create {{ spark_master_service_file }} directory
  file:
    path: "{{ spark_master_service_file }}"
    state: touch
  when: "'spark_masters' in group_names"

- name: Populating the {{ spark_master_service_file  }}
  blockinfile:
    path: "{{ spark_master_service_file }}"
    marker: "# {mark} Spark master service file"
    block: |
          [Unit]
          Description=Spark master service
          After=syslog.target network.target

          [Service]
          Type=forking
          ExecStart={{ start_master_script }} -h {{ hostvars[inventory_hostname]['ansible_default_ipv4']['address'] }}
          ExecStop={{ stop_master_script }}
          User={{ spark_user }}
          Group={{ spark_user }}
          Restart=always

          [Install]
          WantedBy=multi-user.target
    state: present
  when: "'spark_masters' in group_names"

- name: Make sure spark master service is running
  systemd:
    state: restarted
    daemon_reload: yes
    name: spark-master
  when: "'spark_masters' in group_names"
  tags:
    - restart

- name: Create {{ spark_slave_service_file }} directory
  file:
    path: "{{ spark_slave_service_file }}"
    state: touch
  when: "'spark_slaves' in group_names"

- name: Populating the {{ spark_slave_service_file  }}
  blockinfile:
    path: "{{ spark_slave_service_file }}"
    marker: "# {mark} Spark slave service file"
    block: |
          [Unit]
          Description=Spark slave service
          After=syslog.target network.target

          [Service]
          Type=forking
          ExecStart={{ start_slave_script }} spark://{{ hostvars[groups['spark_masters'][0]]['ansible_default_ipv4']['address'] }}:7077 -h {{ hostvars[inventory_hostname]['ansible_default_ipv4']['address'] }}
          ExecStop={{ stop_slave_script }}
          User={{ spark_user }}
          Group={{ spark_user }}
          Restart=always

          [Install]
          WantedBy=multi-user.target
    state: present
  when: "'spark_slaves' in group_names"

- name: Make sure spark slave service is running
  systemd:
    state: restarted
    daemon_reload: yes
    name: spark-slave
  when: "'spark_slaves' in group_names"
  tags:
    - restart
